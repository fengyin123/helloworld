{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Data\n",
      "the shape of train (478138, 27)\n",
      "the shape of test (18371, 26)\n",
      "Start doing preprocessing\n",
      "========================item==========================\n",
      "========================user==========================\n",
      "=====================context==========================\n",
      "=====================shop===============================\n",
      "the shape of train (478138, 45)\n",
      "the shape of test (18371, 45)\n",
      "Preprocessing done and time elapsed 68.02248907089233\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn import preprocessing\n",
    "import warnings\n",
    "import datetime\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import time\n",
    "\n",
    "from utils import raw_data_path, dump_pickle\n",
    "\n",
    "path = '../data/'\n",
    "train_file = 'round1_ijcai_18_train_20180301.txt'\n",
    "test_file = 'round1_ijcai_18_test_a_20180301.txt'\n",
    "\n",
    "train_file.drop_duplicates('instance_id'， inplace=True)\n",
    "test_file.drop_duplicates('instance_id'， inplace=True)\n",
    "\n",
    "# def load_data():\n",
    "#     train = pd.read_table(path + train_file, encoding='utf8', delim_whitespace=True)\n",
    "#     test = pd.read_table(path + train_file, encoding='utf8', delim_whitespace=True)\n",
    "#     df = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "\n",
    "def date_convert(data):\n",
    "    # Transform into datetime format\n",
    "    data['time'] = pd.to_datetime(data.context_timestamp, unit='s')\n",
    "\n",
    "    # transform into Beijing datetime format\n",
    "    data['realtime'] = data['time'].apply(lambda x: x + datetime.timedelta(hours=8))\n",
    "    data['day'] = data['realtime'].dt.day\n",
    "    data['hour'] = data['realtime'].dt.hour\n",
    "    \n",
    "    return data\n",
    "\n",
    "def base_process(data):\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    print(\"========================item==========================\")\n",
    "    # Divided into different category levels and LabelEncoder()\n",
    "    '''\n",
    "    item_id, item_category_list, item_property_list, item_brand_id, item_city_id, \n",
    "    item_price_level, item_sales_level, item_collected_level, item_pv_level\n",
    "    '''\n",
    "    for i in range(1, 3):\n",
    "        data['item_category_list' + str(i)] = lbl.fit_transform(data['item_category_list'].map(\n",
    "            lambda x: str(str(x).split(';')[i]) if len(str(x).split(';')) > i else 'missing'))\n",
    "    del data['item_category_list'] \n",
    "        \n",
    "    for i in range(10):\n",
    "        data['item_property_list' + str(i)] = lbl.fit_transform(data['item_property_list'].map(\n",
    "            lambda x: str(str(x).split(';')[i]) if len(str(x).split(';')) > i else ''))\n",
    "    del data['item_property_list']\n",
    "    # train_item_property = data['item_property_list'].str.split(';', expand=True).add_prefix('item_property_')\n",
    "    # train_item_property.fillna('missing', inplace=True)\n",
    "    # train_item_property = lbl.fit_transform(train_item_property)\n",
    "    # data = pd.concat([data, train_item_property], axis=1)\n",
    "\n",
    "    for col in ['item_id', 'item_brand_id', 'item_city_id']:\n",
    "        data[col] = lbl.fit_transform(data[col])\n",
    "    \n",
    "    # Fill none with mean\n",
    "    data['item_sales_level'][data.item_sales_level==-1] = None\n",
    "    data['item_sales_level'].fillna(data['item_sales_level'].mean(), inplace=True)\n",
    "    \n",
    "    \n",
    "    print(\"========================user==========================\")\n",
    "    # user_gender_id and user_occupation_id should be handled with one-hot\n",
    "    data[data.user_age_level==-1]['user_age_level'] = None\n",
    "    data['user_age_level'].fillna(data['user_age_level'].mode())\n",
    "    data['user_age_level'] = data['user_age_level'].apply(lambda x: x%1000)\n",
    "    \n",
    "    data[data.user_star_level==-1]['user_star_level'] = None\n",
    "    data['user_star_level'].fillna(data['user_star_level'].mean())\n",
    "    data['user_star_level'] = data['user_star_level'].apply(lambda x: x%3000)\n",
    "    \n",
    "    \n",
    "    print(\"=====================context==========================\")\n",
    "    data = date_convert(data)\n",
    "    \n",
    "    for i in range(5):\n",
    "        data['predict_category_property' + str(i)] = lbl.fit_transform(data['predict_category_property'].map(\n",
    "            lambda x: str(str(x).split(';')[i]) if len(str(x).split(';')) > i else ''))\n",
    "    del data['predict_category_property'] \n",
    "        \n",
    "    print(\"=====================shop===============================\")\n",
    "    data['shop_score_service'][data.shop_score_service==-1] = None\n",
    "    data['shop_score_service'].fillna(data['shop_score_service'].mean(), inplace=True)\n",
    "    \n",
    "    data['user_age_level'][data.user_age_level==-1] = None\n",
    "    data['shop_score_delivery'].fillna(data['shop_score_delivery'].mean(), inplace=True)\n",
    "    \n",
    "    data['shop_score_description'][data.shop_score_description==-1] = None\n",
    "    data['shop_score_description'].fillna(data['shop_score_description'].mean(), inplace=True)\n",
    "    \n",
    "    return data\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start = time.time()\n",
    "    print(\"Load Data\")\n",
    "    train = pd.read_table(path + train_file, encoding='utf8', delim_whitespace=True)\n",
    "    test = pd.read_table(path + test_file, encoding='utf8', delim_whitespace=True)\n",
    "    print('the shape of train {}'.format(train.shape))\n",
    "    print('the shape of test {}'.format(test.shape))\n",
    "    len_train = train.shape[0]\n",
    "    df = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "    print(\"Start doing preprocessing\")\n",
    "    df = base_process(df)\n",
    "    dump_pickle(df, path=raw_data_path + 'df.pkl')\n",
    "    train = df[(df['day'] >= 18) & (df['day'] <= 24)]\n",
    "    print('the shape of train {}'.format(train.shape))\n",
    "    dump_pickle(train, path=raw_data_path + 'train.pkl')\n",
    "    test = df.iloc[len_train:]\n",
    "    print('the shape of test {}'.format(test.shape))\n",
    "    dump_pickle(test, path=raw_data_path + 'test.pkl')\n",
    "    \n",
    "    end = time.time()\n",
    "    print(\"Preprocessing done and time elapsed %s\" % (end-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "478138   NaN\n",
       "478139   NaN\n",
       "478140   NaN\n",
       "478141   NaN\n",
       "478142   NaN\n",
       "478143   NaN\n",
       "478144   NaN\n",
       "478145   NaN\n",
       "478146   NaN\n",
       "478147   NaN\n",
       "478148   NaN\n",
       "478149   NaN\n",
       "478150   NaN\n",
       "478151   NaN\n",
       "478152   NaN\n",
       "478153   NaN\n",
       "478154   NaN\n",
       "478155   NaN\n",
       "478156   NaN\n",
       "478157   NaN\n",
       "478158   NaN\n",
       "478159   NaN\n",
       "478160   NaN\n",
       "478161   NaN\n",
       "478162   NaN\n",
       "478163   NaN\n",
       "478164   NaN\n",
       "478165   NaN\n",
       "478166   NaN\n",
       "478167   NaN\n",
       "          ..\n",
       "496479   NaN\n",
       "496480   NaN\n",
       "496481   NaN\n",
       "496482   NaN\n",
       "496483   NaN\n",
       "496484   NaN\n",
       "496485   NaN\n",
       "496486   NaN\n",
       "496487   NaN\n",
       "496488   NaN\n",
       "496489   NaN\n",
       "496490   NaN\n",
       "496491   NaN\n",
       "496492   NaN\n",
       "496493   NaN\n",
       "496494   NaN\n",
       "496495   NaN\n",
       "496496   NaN\n",
       "496497   NaN\n",
       "496498   NaN\n",
       "496499   NaN\n",
       "496500   NaN\n",
       "496501   NaN\n",
       "496502   NaN\n",
       "496503   NaN\n",
       "496504   NaN\n",
       "496505   NaN\n",
       "496506   NaN\n",
       "496507   NaN\n",
       "496508   NaN\n",
       "Name: is_trade, Length: 18371, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['is_trade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
