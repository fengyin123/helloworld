{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 14%|█▍        | 1/7 [00:11<01:06, 11.12s/it]\u001b[A\n",
      " 29%|██▊       | 2/7 [00:23<00:58, 11.80s/it]\u001b[A\n",
      " 43%|████▎     | 3/7 [00:36<00:48, 12.14s/it]\u001b[A\n",
      " 57%|█████▋    | 4/7 [00:49<00:37, 12.41s/it]\u001b[A\n",
      " 71%|███████▏  | 5/7 [01:03<00:25, 12.63s/it]\u001b[A\n",
      " 86%|████████▌ | 6/7 [01:16<00:12, 12.74s/it]\u001b[A\n",
      "100%|██████████| 7/7 [01:30<00:00, 12.90s/it]\u001b[A\n",
      " 33%|███▎      | 1/3 [01:30<03:00, 90.41s/it]\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 14%|█▍        | 1/7 [00:00<00:00,  8.70it/s]\u001b[A\n",
      " 29%|██▊       | 2/7 [00:00<00:00,  7.14it/s]\u001b[A\n",
      " 43%|████▎     | 3/7 [00:00<00:00,  6.37it/s]\u001b[A\n",
      " 57%|█████▋    | 4/7 [00:00<00:00,  5.71it/s]\u001b[A\n",
      " 71%|███████▏  | 5/7 [00:00<00:00,  5.16it/s]\u001b[A\n",
      " 86%|████████▌ | 6/7 [00:01<00:00,  4.86it/s]\u001b[A\n",
      "100%|██████████| 7/7 [00:01<00:00,  4.54it/s]\u001b[A\n",
      " 67%|██████▋   | 2/3 [01:32<00:46, 46.02s/it]\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 14%|█▍        | 1/7 [00:00<00:00,  9.38it/s]\u001b[A\n",
      " 29%|██▊       | 2/7 [00:00<00:00,  8.09it/s]\u001b[A\n",
      " 43%|████▎     | 3/7 [00:00<00:00,  6.89it/s]\u001b[A\n",
      " 57%|█████▋    | 4/7 [00:00<00:00,  6.20it/s]\u001b[A\n",
      " 71%|███████▏  | 5/7 [00:00<00:00,  5.63it/s]\u001b[A\n",
      " 86%|████████▌ | 6/7 [00:01<00:00,  5.16it/s]\u001b[A\n",
      "100%|██████████| 7/7 [00:01<00:00,  4.72it/s]\u001b[A\n",
      "100%|██████████| 3/3 [01:33<00:00, 31.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['instance_id', 'shop_id_smooth_rate',\n",
      "       'shop_review_num_level_smooth_rate', 'shop_star_level_smooth_rate'],\n",
      "      dtype='object')\n",
      "the shape of train (478087, 4)\n",
      "the shape of test (42888, 4)\n"
     ]
    }
   ],
   "source": [
    "# %load 102_user_smooth.py\n",
    "from utils import BayesianSmoothing, load_pickle, dump_pickle, raw_data_path\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# 载入数据\n",
    "train = load_pickle('../data/train.pkl')\n",
    "test = load_pickle('../data/test.pkl')\n",
    "df = pd.concat([train, test], axis=0)\n",
    "\n",
    "# 贝叶斯平滑参数\n",
    "iter_num = 1000\n",
    "epsilon = 0.001\n",
    "\n",
    "'''\n",
    "1. 定义需要计算平滑点击率的变量\n",
    "2. 对于每一天，找出在这之前的所有点击行为\n",
    "3. 统计该变量的点击次数和购买次数\n",
    "'''\n",
    "\n",
    "smooth_cols = ['shop_id', 'shop_review_num_level', 'shop_star_level']\n",
    "\n",
    "# 保存最后结果的dataframe\n",
    "smooth_train = train[smooth_cols + ['instance_id', 'day']]\n",
    "smooth_test = test[smooth_cols + ['instance_id','day']]\n",
    "\n",
    "# 开始对每个特征进行贝叶斯平滑以获取历史交易率\n",
    "for col in tqdm(smooth_cols):\n",
    "    # 定义特征名\n",
    "    col_I = '{}_I'.format(col)\n",
    "    col_C = '{}_C'.format(col)\n",
    "    col_smooth_rate = '{}_smooth_rate'.format(col)\n",
    "    \n",
    "#     train[col_smooth_rate] = -1\n",
    "#     smooth_all = pd.DataFrame({'day': train.day, '{}'.format(col): train[col]})\n",
    "    CVR_all = None\n",
    "    for day in tqdm(range(19, 26)):\n",
    "        # 统计总浏览数和购买数\n",
    "        I = train[train.day<day].groupby(col)['is_trade'].count().reset_index()\n",
    "        I.columns = [col, col_I]\n",
    "        C = train[train.day<day].groupby(col)['is_trade'].sum().reset_index()\n",
    "        C.columns = [col, col_C]\n",
    "        CVR = pd.concat([I, C[col_C]], axis=1)\n",
    "        \n",
    "        # CVR的columns：[col, col_I, col_C, 'day']\n",
    "        CVR['day'] = day\n",
    "        \n",
    "        # 贝叶斯平滑过程\n",
    "        smooth = BayesianSmoothing(1, 1)\n",
    "        smooth.update(CVR[col_I].values, CVR[col_C].values, iter_num, epsilon)\n",
    "        alpha = smooth.alpha\n",
    "        beta = smooth.beta\n",
    "        CVR[col_smooth_rate] = (CVR[col_C] + alpha) / (CVR[col_I] + alpha + beta)\n",
    "        \n",
    "        # 把不同天算的concat起来\n",
    "        CVR_all = pd.concat([CVR_all, CVR], axis=0)\n",
    "        # print(CVR.head())\n",
    "        # smooth_all[col_smooth_rate] = -1\n",
    "        # print((pd.merge(train[train.day == day], CVR[[col, col_smooth_rate]], on=col, how='inner')).columns[-1])\n",
    "        # smooth_all[col_smooth_rate][smooth_all.day == day] = (pd.merge(train[train.day == day], CVR[[col, col_smooth_rate]], on=col, how='left')).iloc[:, -1].values\n",
    "\n",
    "    smooth_train = pd.merge(smooth_train, CVR_all[[col, 'day', col_smooth_rate]], on=[col, 'day'], how='left')\n",
    "    smooth_test = pd.merge(smooth_test, CVR_all[[col, 'day', col_smooth_rate]], on=[col, 'day'], how='left')\n",
    "\n",
    "\n",
    "smooth_train.drop(['day'] + smooth_cols, axis=1, inplace=True)\n",
    "smooth_test.drop(['day'] + smooth_cols, axis=1, inplace=True)\n",
    "print(smooth_train.columns)\n",
    "print('the shape of train {}'.format(smooth_train.shape))\n",
    "print('the shape of test {}'.format(smooth_test.shape))\n",
    "dump_pickle(smooth_train, path='../data/train_feature/202_smooth_features.pkl')\n",
    "dump_pickle(smooth_test, path='../data/test_feature/202_smooth_features.pkl')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
