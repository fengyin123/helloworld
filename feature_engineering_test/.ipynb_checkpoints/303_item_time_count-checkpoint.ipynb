{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item \n",
    "from utils import BayesianSmoothing, load_pickle, dump_pickle, raw_data_path\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "df = load_pickle('../data/df.pkl')\n",
    "train = load_pickle('../data/train.pkl')\n",
    "test = load_pickle('../data/test.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(478087, 36) (18371, 36)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape,test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_time_count(df, mode):\n",
    "    # ========================= 产品当天被搜索次数和当前小时被搜索次数 =========================================\n",
    "    time_features = ['hour', 'day']\n",
    "    item_features = ['item_id', 'item_brand_id', 'item_city_id', 'item_price_level', 'item_sales_level','item_collected_level','item_pv_level']\n",
    "\n",
    "    item_time_df = pd.DataFrame()\n",
    "    item_time_df['instance_id'] = df['instance_id']\n",
    "\n",
    "    for item_feature in tqdm(item_features):\n",
    "        for time_feature in time_features:\n",
    "            search_group = df.groupby([item_feature, time_feature]).count().reset_index()\n",
    "            tmp_df = df[[item_feature, time_feature]]\n",
    "            item_day_search = pd.merge(tmp_df, search_group, on=[item_feature, time_feature], how='left').iloc[:, -1]\n",
    "            item_time_df['{}_{}_search'.format(item_feature, time_feature)] = item_day_search\n",
    "    print(item_time_df.columns)\n",
    "    print('the shape of {} {}'.format(mode, item_time_df.shape))\n",
    "    dump_pickle(item_time_df, path=raw_data_path+'{}_feature/'.format(mode)+'302_item_time_count.pkl')\n",
    "    \n",
    "    \n",
    "    # ======================== 产品当前被搜索距离上次的时间 ================================================\n",
    "    df_tmp = df[['instance_id', 'item_id',  'context_timestamp']].copy()\n",
    "    df_tmp.sort_values(['item_id', 'context_timestamp'], inplace=True)\n",
    "\n",
    "    df_tmp['item_t-1_context_timestamp'] = df_tmp.groupby('item_id')['context_timestamp'].shift(1)\n",
    "    df_tmp['item_time_diff_last_query'] = df_tmp['context_timestamp'] - df_tmp['item_t-1_context_timestamp']\n",
    "    # df_tmp['item_time_diff_last_query'] = np.log1p(df_tmp['context_timestamp'] - df_tmp['item_t-1_context_timestamp'])\n",
    "\n",
    "    final_feat = df_tmp[['instance_id', 'item_time_diff_last_query']]\n",
    "    print(final_feat.columns)\n",
    "    print('the shape of {} {}'.format(mode, final_feat.shape))\n",
    "    dump_pickle(final_feat, path=raw_data_path+'{}_feature/'.format(mode)+'302_item_last_query.pkl')\n",
    "    \n",
    "    \n",
    "        # ========================= 当日当前搜索距离上次的时间(商品，商标) ==========================================\n",
    "    final_feat = pd.DataFrame()\n",
    "    final_feat['instance_id'] = df['instance_id']\n",
    "    cols = ['item_id', 'item_brand_id']\n",
    "    for col in tqdm(cols):\n",
    "        df_select = df[[col,'day','context_timestamp']]\n",
    "        df_group =  df_select.groupby([col,'day'])\n",
    "        group_max = df_group['context_timestamp'].transform('max')\n",
    "        group_min = df_group['context_timestamp'].transform('min')\n",
    "        final_feat['diff_maxtime_{}'.format(col)] = group_max - df['context_timestamp'].values\n",
    "        final_feat['diff_mintime_{}'.format(col)] = df['context_timestamp'].values -group_min\n",
    "    print(final_feat.columns)\n",
    "    print('the shape of {} {}'.format(mode, final_feat.shape))\n",
    "    dump_pickle(final_feat, path=raw_data_path + '{}_feature/'.format(mode) + '302_item_diff_max_min.pkl')\n",
    "    \n",
    "    \n",
    "    # ================================= 当前日期前一天的cnt ===========================================\n",
    "    count_features = ['item_brand_id', 'item_city_id', 'item_price_level', 'item_sales_level','item_collected_level','item_pv_level']\n",
    "    final_feat = df[count_features+['instance_id', 'day']]\n",
    "    for col in count_features:\n",
    "        count_name = '{}_lastday_count'.format(col)\n",
    "        count_all = None\n",
    "        for d in range(18, 24):\n",
    "            col_cnt = df[df['day'] == d - 1].groupby(by=col)['instance_id'].count().reset_index()\n",
    "            col_cnt.columns = [col, count_name]\n",
    "            col_cnt['day'] = d\n",
    "            count_all = pd.concat([count_all, col_cnt], axis=0)\n",
    "        final_feat = pd.merge(final_feat, count_all, on=[col, 'day'], how='left')\n",
    "    final_feat = final_feat.drop(count_features+['day'], axis=1)\n",
    "    print(final_feat.columns)\n",
    "    print('the shape of {} {}'.format(mode, final_feat.shape))\n",
    "\n",
    "    dump_pickle(final_feat, path=raw_data_path + '{}_feature/'.format(mode) + '302_item_last_day_count.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:05<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['instance_id', 'item_id_hour_search', 'item_id_day_search',\n",
      "       'item_brand_id_hour_search', 'item_brand_id_day_search',\n",
      "       'item_city_id_hour_search', 'item_city_id_day_search',\n",
      "       'item_price_level_hour_search', 'item_price_level_day_search',\n",
      "       'item_sales_level_hour_search', 'item_sales_level_day_search',\n",
      "       'item_collected_level_hour_search', 'item_collected_level_day_search',\n",
      "       'item_pv_level_hour_search', 'item_pv_level_day_search'],\n",
      "      dtype='object')\n",
      "the shape of train (478087, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['instance_id', 'item_time_diff_last_query'], dtype='object')\n",
      "the shape of train (478087, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 12.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['instance_id', 'diff_maxtime_item_id', 'diff_mintime_item_id',\n",
      "       'diff_maxtime_item_brand_id', 'diff_mintime_item_brand_id'],\n",
      "      dtype='object')\n",
      "the shape of train (478087, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['instance_id', 'item_brand_id_lastday_count',\n",
      "       'item_city_id_lastday_count', 'item_price_level_lastday_count',\n",
      "       'item_sales_level_lastday_count', 'item_collected_level_lastday_count',\n",
      "       'item_pv_level_lastday_count'],\n",
      "      dtype='object')\n",
      "the shape of train (478087, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 23.69it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 137.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['instance_id', 'item_id_hour_search', 'item_id_day_search',\n",
      "       'item_brand_id_hour_search', 'item_brand_id_day_search',\n",
      "       'item_city_id_hour_search', 'item_city_id_day_search',\n",
      "       'item_price_level_hour_search', 'item_price_level_day_search',\n",
      "       'item_sales_level_hour_search', 'item_sales_level_day_search',\n",
      "       'item_collected_level_hour_search', 'item_collected_level_day_search',\n",
      "       'item_pv_level_hour_search', 'item_pv_level_day_search'],\n",
      "      dtype='object')\n",
      "the shape of test (18371, 15)\n",
      "Index(['instance_id', 'item_time_diff_last_query'], dtype='object')\n",
      "the shape of test (18371, 2)\n",
      "Index(['instance_id', 'diff_maxtime_item_id', 'diff_mintime_item_id',\n",
      "       'diff_maxtime_item_brand_id', 'diff_mintime_item_brand_id'],\n",
      "      dtype='object')\n",
      "the shape of test (18371, 5)\n",
      "Index(['instance_id', 'item_brand_id_lastday_count',\n",
      "       'item_city_id_lastday_count', 'item_price_level_lastday_count',\n",
      "       'item_sales_level_lastday_count', 'item_collected_level_lastday_count',\n",
      "       'item_pv_level_lastday_count'],\n",
      "      dtype='object')\n",
      "the shape of test (18371, 7)\n",
      "time elapsed 8.118268013000488\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "item_time_count(train, 'train')\n",
    "item_time_count(test, 'test')\n",
    "end = time.time()\n",
    "print('time elapsed {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
