{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 29%|██▊       | 2/7 [00:00<00:00, 10.20it/s]\u001b[A\n",
      " 43%|████▎     | 3/7 [00:00<00:00,  8.68it/s]\u001b[A\n",
      " 57%|█████▋    | 4/7 [00:00<00:00,  7.70it/s]\u001b[A\n",
      " 71%|███████▏  | 5/7 [00:00<00:00,  6.51it/s]\u001b[A\n",
      " 86%|████████▌ | 6/7 [00:01<00:00,  5.70it/s]\u001b[A\n",
      "100%|██████████| 7/7 [00:01<00:00,  5.34it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:01<00:04,  1.40s/it]\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 29%|██▊       | 2/7 [00:00<00:00,  9.60it/s]\u001b[A\n",
      " 43%|████▎     | 3/7 [00:00<00:00,  8.13it/s]\u001b[A\n",
      " 57%|█████▋    | 4/7 [00:00<00:00,  7.09it/s]\u001b[A\n",
      " 71%|███████▏  | 5/7 [00:00<00:00,  6.31it/s]\u001b[A\n",
      " 86%|████████▌ | 6/7 [00:01<00:00,  5.55it/s]\u001b[A\n",
      "100%|██████████| 7/7 [00:01<00:00,  5.15it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [00:02<00:02,  1.44s/it]\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 29%|██▊       | 2/7 [00:00<00:00,  8.27it/s]\u001b[A\n",
      " 43%|████▎     | 3/7 [00:00<00:00,  6.66it/s]\u001b[A\n",
      " 57%|█████▋    | 4/7 [00:00<00:00,  5.63it/s]\u001b[A\n",
      " 71%|███████▏  | 5/7 [00:00<00:00,  5.24it/s]\u001b[A\n",
      " 86%|████████▌ | 6/7 [00:01<00:00,  4.79it/s]\u001b[A\n",
      "100%|██████████| 7/7 [00:01<00:00,  4.62it/s]\u001b[A\n",
      " 75%|███████▌  | 3/4 [00:04<00:01,  1.50s/it]\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 14%|█▍        | 1/7 [00:00<00:00,  9.73it/s]\u001b[A\n",
      " 29%|██▊       | 2/7 [00:00<00:00,  8.07it/s]\u001b[A\n",
      " 43%|████▎     | 3/7 [00:00<00:00,  6.90it/s]\u001b[A\n",
      " 57%|█████▋    | 4/7 [00:00<00:00,  6.06it/s]\u001b[A\n",
      " 71%|███████▏  | 5/7 [00:00<00:00,  5.49it/s]\u001b[A\n",
      " 86%|████████▌ | 6/7 [00:01<00:00,  5.18it/s]\u001b[A\n",
      "100%|██████████| 7/7 [00:01<00:00,  5.15it/s]\u001b[A\n",
      "100%|██████████| 4/4 [00:05<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['instance_id', 'user_gender_id_smooth_rate',\n",
      "       'user_age_level_smooth_rate', 'user_occupation_id_smooth_rate',\n",
      "       'user_star_level_smooth_rate'],\n",
      "      dtype='object')\n",
      "the shape of train (420676, 5)\n",
      "the shape of test (57411, 5)\n"
     ]
    }
   ],
   "source": [
    "# %load 102_user_smooth.py\n",
    "from utils import BayesianSmoothing, load_pickle, dump_pickle, raw_data_path\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# 载入数据\n",
    "train = load_pickle('../data/train.pkl')\n",
    "test = load_pickle('../data/test.pkl')\n",
    "df = pd.concat([train, test], axis=0)\n",
    "\n",
    "# 贝叶斯平滑参数\n",
    "iter_num = 1000\n",
    "epsilon = 0.001\n",
    "\n",
    "'''\n",
    "1. 定义需要计算平滑点击率的变量\n",
    "2. 对于每一天，找出在这之前的所有点击行为\n",
    "3. 统计该变量的点击次数和购买次数\n",
    "'''\n",
    "\n",
    "# smooth_cols = ['user_id', 'user_gender_id', 'user_age_level', 'user_occupation_id', 'user_star_level']\n",
    "smooth_cols = ['user_gender_id', 'user_age_level', 'user_occupation_id', 'user_star_level']\n",
    "\n",
    "# 保存最后结果的dataframe\n",
    "smooth_train = train[smooth_cols + ['instance_id', 'day']]\n",
    "smooth_test = test[smooth_cols + ['instance_id','day']]\n",
    "\n",
    "# 开始对每个特征进行贝叶斯平滑以获取历史交易率\n",
    "for col in tqdm(smooth_cols):\n",
    "    # 定义特征名\n",
    "    col_I = '{}_I'.format(col)\n",
    "    col_C = '{}_C'.format(col)\n",
    "    col_smooth_rate = '{}_smooth_rate'.format(col)\n",
    "    \n",
    "#     train[col_smooth_rate] = -1\n",
    "#     smooth_all = pd.DataFrame({'day': train.day, '{}'.format(col): train[col]})\n",
    "    CVR_all = None\n",
    "    for day in tqdm(range(19, 26)):\n",
    "        # 统计总浏览数和购买数\n",
    "        I = train[train.day<day].groupby(col)['is_trade'].count().reset_index()\n",
    "        I.columns = [col, col_I]\n",
    "        C = train[train.day<day].groupby(col)['is_trade'].sum().reset_index()\n",
    "        C.columns = [col, col_C]\n",
    "        CVR = pd.concat([I, C[col_C]], axis=1)\n",
    "        \n",
    "        # CVR的columns：[col, col_I, col_C, 'day']\n",
    "        CVR['day'] = day\n",
    "        \n",
    "        # 贝叶斯平滑过程\n",
    "        smooth = BayesianSmoothing(1, 1)\n",
    "        smooth.update(CVR[col_I].values, CVR[col_C].values, iter_num, epsilon)\n",
    "        alpha = smooth.alpha\n",
    "        beta = smooth.beta\n",
    "        CVR[col_smooth_rate] = (CVR[col_C] + alpha) / (CVR[col_I] + alpha + beta)\n",
    "        \n",
    "        # 把不同天算的concat起来\n",
    "        CVR_all = pd.concat([CVR_all, CVR], axis=0)\n",
    "\n",
    "    smooth_train = pd.merge(smooth_train, CVR_all[[col, 'day', col_smooth_rate]], on=[col, 'day'], how='left')\n",
    "    smooth_test = pd.merge(smooth_test, CVR_all[[col, 'day', col_smooth_rate]], on=[col, 'day'], how='left')\n",
    "\n",
    "\n",
    "smooth_train.drop(['day'] + smooth_cols, axis=1, inplace=True)\n",
    "smooth_test.drop(['day'] + smooth_cols, axis=1, inplace=True)\n",
    "print(smooth_train.columns)\n",
    "print('the shape of train {}'.format(smooth_train.shape))\n",
    "print('the shape of test {}'.format(smooth_test.shape))\n",
    "dump_pickle(smooth_train, path='../data/train_feature/102_smooth_features.pkl')\n",
    "dump_pickle(smooth_test, path='../data/test_feature/102_smooth_features.pkl')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instance_id                       0.000000\n",
       "user_gender_id_smooth_rate        0.186036\n",
       "user_age_level_smooth_rate        0.186036\n",
       "user_occupation_id_smooth_rate    0.186036\n",
       "user_star_level_smooth_rate       0.186036\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smooth_train.isnull().sum(axis=0)/smooth_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
